% !TEX program = xelatex
\documentclass[11pt,a4paper]{article}
\usepackage{polyglossia}
\usepackage{fontspec}
\usepackage{subfig}
\usepackage{url}
\usepackage{hyperref}
\usepackage[round]{natbib}

% natbib link joining; somewhat breaks \cite, \citet
\makeatletter
\renewcommand\hyper@natlinkbreak[2]{#1}
\makeatother

\usepackage{geometry}
\geometry{%
	includeheadfoot,
	margin=1in
}

\def\TODO{{\bf ??? }}

\title{Argus: Deciding Questions about Events \\ (working paper) [DRAFT]}
\author{Petr Baudiš \\ Ailao}

\begin{document}
\maketitle

\begin{abstract}%
	We consider the problem of reliably answering yes/no questions
	about events based on news sources in settings of the Argus QA
	system and the Syphon question pre-validator.
	We give a brief overview of the relevant state-of-art methods
	in the academic field of Natural Language Processing,
	requirements for building a solid state-of-art system
	and difficulties we may anticipate in the road ahead.
	Based on this, we propose a basic architecture of the Argus
	system, roadmap to achieve baseline performance, and a basic
	version of the Syphon system.
\end{abstract}

\vspace{3ex}

{\itshape

In the working paper you should be sure to highlight the key aspects of uncertainty and anticipated difficulty.
	
And also, of course, the need for +alpha development - to whatever extent you
can detail what specifically is known and unknown that future development would
help clarify. Bringing in +that recent Cornell research might also be helpful
if you consider it valid (I know you were kind of skeptical of it).

Another thing I neglected to mention - I know they're not exactly the same, but it would also be very good to whatever extent you can to
point to YodaQA as a kind of preliminary proof of concept and include any mention you can of the lessons learned from building that and how those
lessons would/might come in to play with building the "yes/no/unclear" Argus option.
}

Our team at the Ailao startup, based at the 3C Group FEE CTU,
is composed of people working at the boundary of academic and commercial
sector and with a history of projects that involve
various aspects of Natural Language Processing ---
information retrieval, web scraping and information extraction,
deep learning and, most prominently in this context, question answering.
Our flagship project is the YodaQA system for answering factoid questions,
but we also built systems for automatic tagging of newsreel messages
or answering multiple-choice entrance exam type questions.

A natural question is whether we could simply apply our YodaQA system
on this task.  However, while there are some common analysis elements,
generating factoid answers is programatically quite a different task
from deciding whether a sentence is correct or incorrect.%
\footnote{A baseline support for answering yes / no questions is currently
under development within YodaQA as a student internship project, but it is
not finished yet and not meant to be highly reliable at this point.
The initial proposed method is at
\url{https://github.com/brmson/yodaqa/blob/f/textent/doc/ENTAILMENT.md}
but the difference from our scenario is that we are answering questions
on top of a fixed knowledge base, mainly structured one, rather than an
evolving news stream.  The baseline techniques are also so simple that
reusing implementation wouldn't help that much --- but reusing experience
definitely will.}
We believe the most efficient approach to take for a prototype is thus
to build a system from scratch, reusing mainly our expertise and possibly
some isolated external components (e.g.\ database interfaces or NLP annotators).

The rest of the paper is structured as follows.
In Sec.~\ref{qaml}, we rehash what does it involve to build Natural Language Processing
system that uses machine learning and is rigorously developed and measured.
In Sec.~\ref{ynml}, we look at current scientific concepts that relate
to the yes/no QA task at hand, and at their performance.
In Sec.~\ref{structure}, we explore the yes/no QA task in more detail,
identifying different types of questions and how to tackle each.
As a corollary, we explain how to sort the easy from the difficult
even before attempting an answer in Sec.~\ref{syphon}.
We sketch the system and its main concrete strategies based on these
ingredients in Sec.~\ref{system}.
Lastly in the Sec.~\ref{infra}, we give an overview of
the boring but most essential part of the system --- the infrastructure
that collects and manages our data, runs the experiments and interacts with the user.

\section{QA System in Machine Learning Context}
\label{qaml}

When building an NLP system that uses machine learning components,
we need a rigorous way to (i) train these components, and (ii) evaluate
the system performance.

Our NLP system is a \textit{classifier}, i.e.\ a program that takes
a sentence (and a large knowledge base) and classifies it as either
true or false.  The typical approach in such a scenario is building
a \textit{gold standard} dataset --- a set of questions, each annotated
with the correct answer (yes, no, unknown) and an accompanying knowledge
base that should, on its own, lend support to deciding these questions.%
\footnote{With a large enough knowledge base, this issue tends to vanish,
of course.}

In general, the bigger the dataset, the better.  Commonly used datasets
range in size from low hundreds%
\footnote{E.g.\ the \textit{Curated TREC QA Track Dataset} \url{https://github.com/brmson/dataset-curated-factoid/}}
to tens of thousands%
\footnote{E.g.\ the \textit{Large Movie Review Dataset} \url{http://ai.stanford.edu/~amaas/data/sentiment/}}
but we can certainly grow the dataset as needed and we will be able to identify
insufficient dataset size as a possible improvement factor during development.
We should start with at least 100 questions, for initial evaluation of our
baseline.

These questions are further split, at the very least to a \textit{training}
set and a \textit{testing} set (more fine splits are often desirable
with enough questions available).
To train even simple machine learning models, we should try hard to get
at least 300 questions, but to train some more advanced models, we need
to aim at figures like 6000 questions.

To get the dataset, the simplest option is to hire low-paid virtual assistants
to build it.  Aside of using common employment platforms, Amazon runs a
so-called Mechanical Turk service that allow humans to be employed easily
for these kinds of task.  Finally, it might be possible to build the dataset
semi-automatically by using some clever approaches --- this makes it easy
to build a large dataset, but holds the danger of noise (non-sensical questions
or wrong reference answers) and drifting off from realistic questions the
users will pose.  It would be an interesting problem / milestone on its own,
but something we might have to tackle.

Many of the models need another kind of dataset as well --- pairs of question
and news sentence that are annotated by whether the sentence carries the yes/no
answer for the question.  However, from a purely question-based dataset,
generating a medium-quality dataset like this should be straightforward,
and reviewing predefined annotations by humans is much easier and faster
than creating the annotations in the first place.

\textit{Our project proposal assumed that you would deliver such a dataset.
	We can certainly also arrange building such a dataset, in the vein
	of future-looking questions you have provided us, as an additionally
budgeted milestone.}

The main criterium for evaluating our system is obviously \textit{accuracy},
that is the percentage of questions our system gets right.  However, this
number does not capture questions where the system ``gives up'' and says
unknown; one possibility is to use a harmonic average of (i) the percentage
of questions the system gets right when it decides to answer, and (ii) the
percentage of questions the system decides to answer.%
\footnote{This combined percentage is called \textit{F1 score} in machine learning jargon.}
We can always change the threshold of confidence the system uses to decide
whether to answer to keep the accuracy of answered questions at the desired
score like 95\%, the caveat is that the system may refuse to answer almost
all questions at that point; therefore, this average metric is a better
option for judging the system performance.

Performance evaluation in the machine learning context has two clear limitations
that we are not sure how to overcome right now.  First, it is not clear how to
effectively evaluate Syphon performance when users are repeatedly adjusting
their queries to force them through; we'll come back to this below.
Second, we assume a \textit{non-adversarial scenario} where user questions
are expected to be unbiased and users are not actively trying to trick the system;
this assumption is not realistic, but we are not aware of any good answers science
has to this problem.  Out-of-the-loop safeguards like random manual review,
adaptive Syphon or large-scale consensus of diverse automata might be required.
Another possibility is that a group of ``white hat'' users will counter-analyze
trending questions for possible flaws of this nature and natural balance will
emerge in the ecosystem --- the motivation of these users being reputation and
credentials (user rating might be an excellent base for getting hired as a
proprietary analyst), or even bounties for bad questions.

\section{Machine Learning for Answering Yes/No Questions}
\label{ynml}

Preface: The success of vector embeddings, word2vec.
However, named entity troubles.

yes/no questions == text entailment (of question and its negation).

\textsc{Excitement}

sentiment analysis

answering sentence selection

comprehension (entrance exam) tasks

\section{Structure of Our Problem}
\label{structure}

\begin{figure}
\begin{verbatim}
Did Hillary Clinton become Presidemt?
Did Scott Walker become US President?
Did Martin O’Malley become US President?
Was Obamacare dismantled by the Supreme Court?
Was Saudi Arabia’s monarchy be dethroned in a coup d’etat?
Did Iran test detonate a nuclear bomb"
Did Israel attack Iran?
Did Germany ban the PEDIGA movement?
Did the US dollar collapse by at least 50\%?
Did the euro collapse another 50\%?
Did Germany abandon the euro?
Did the EU slide into a new recession?
Was a miracle cure for diabetes discovered?
Did a real blizzard lead to power cuts for more than 10 million Americans?
Did Sears declare bankruptcy?
\end{verbatim}
	\caption{Sample questions as provided by the customer.}
	\label{fig:sampleq}
\end{figure}

Clearly, not all yes/no questions are created equal:
Some are easy to understand, others are hard.
Some are easy to find an answers, others will produce just a smidgen of mentions in the knowledge base.
Some are quantitatively unclear and answers might be triggerred by published subjective opinions.
Let us explore what these distinctions mean,
since recognizing them and rejecting answering
the hard questions outright is our key to achieving a high precision.
Fig.~\ref{fig:sampleq} contains a list of sample questions (forward-looking as of now).

Let us group the questions to several classes in terms of how easy to automatically judge they are:

\begin{itemize}
	\item Clear-cut questions: \textbf{Did X Y become President?},
		\textbf{Did Iran test detonate a nuclear bomb?},
		\textbf{Did Germany ban the PEDIGA movement?}

		These should be generally easy to parse by Syphon
		and to answer unambiguously from news sources.

	\item Fuzzy-matching questions: \textbf{Was Saudi Arabia’s monarchy be dethroned in a coup d’etat?}
		\textbf{Did Germany abandon the euro?},
		\textbf{Did Sears declare bankruptcy?}

		These are about unambiguous facts and should pass Syphon,
		but simple text matching strategies might fail when
		answering the question - the system needs to match
		``revolution'' to ``dethroned in a coup d'etat'',
		many news titles would use ``leave the eurozone''
		instead of ``abandon the euro'' and ``declare bankrupcy''
		is the same as e.g.\ ``go bankrupt''.

		Based on results like TODO,
		we can predict that many of the simpler paraphrases
		would be covered by vector embeddings%
\footnote{There is also a Wordnet dictionary that covers many synonymic relationships of individual words, but extending it to verb/noun combinations is less trivial.}
		but this is unlikely to solve the ``abandon the euro''.
		We can either rely on stricter Syphon
		(we could suggest templates like ``starts'' and ``stops''
		together with knowledge base attributes),
		or simply hope that the newspapers will use so varied
		phrasings that the one particularly in the question
		will still appear often enough; the alpha prototype
		should shed more light on this.

		Also, another tricky part is that \textbf{Sears} is name
		of many legal entities; do we mean the department store
		chain, \textbf{Sears Holdings} or some sister company?
		However, Syphon can catch ambiguous references like this.

	\item Questions requiring inference: \textbf{Did the US dollar collapse by at least 50\%?},
		\textbf{Did the euro collapse another 50\%?},
		\textbf{Did a real blizzard lead to power cuts for more than 10 million Americans?}

		Answering these questions with simple text matching
		is unlikely to produce any results.  Syphon shouldn't
		be able to decompose this question to simple elements
		and relationships between these and so shouldn't let
		pass these questions through.

	\item Subjective questions: \textbf{Was Obamacare dismantled by the Supreme Court?},
		\textbf{Did Israel attack Iran?},
		\textbf{Did the EU slide into a new recession?},
		\textbf{Was a miracle cure for diabetes discovered?}

		These questions are particularly tricky as filtering
		them with Syphon may be problematic.  Entity X attacking
		entity Y seems quite unambiguous and well-defined.
		On the other hand, the definition of ``attack'' may
		vary widely\footnote{Israel is a good example as the country is known for executing military and intelligence operations covertly and avoiding admission of involvement.} --- from bombing of specific site
		to a full-scale military attack.

		We don't have a good solution to this class of questions,
		except warning users that the act of ``dismantling''
		simply means that enough commentators consider the court
		judgement to be this, the same with ``recession'' or what
		is a ``miracle cure'' (we already have several, or none).
\end{itemize}

data sources: news sites, knowledge bases (wikidata).

\section{How Can We Know What Do We Know?}
\label{syphon}

syphon brief outline.

two roles: reject bad entities, provide system interpretation and show
it along with the question for other users.

named entity search method: named entities, wikipedia aliases

clearly identify concepts, relationships.  in case of doubt, reject.

output: set of named entities or concept vectors, relationship words.

\section{A Concrete Proposal}
\label{system}

Concrete system proposal: Syphon that detects named entities (enwiki titles) and verbs, rejects if anything extra is present.
Get pre-processed version from syphon.
Scan news titles (and possibly structured databases), model entailment (yes, unknown, now) for each,
use relative counts (normalized by general named entity occurrence) to estimate probability.

initial knowledge base: guardian api? wikidata?

baseline entailment method: bag-of-words, use Wordnet synsets.
Deep anssentsel would be easy, but needs a dataset of at least a few thousand question, sentence pairs - use multitask learning?

\section{Infrastructure}
\label{infra}

technical details --- python, database used, etc.

news gathering etc.

distributed system prospects.

\bibliographystyle{plainnat}
\bibliography{qa}

\end{document}
