% !TEX program = xelatex
\documentclass[11pt,a4paper]{article}
\usepackage{polyglossia}
\usepackage{fontspec}
\usepackage{subfig}
\usepackage{url}
\usepackage{hyperref}
\usepackage[round]{natbib}

% natbib link joining; somewhat breaks \cite, \citet
\makeatletter
\renewcommand\hyper@natlinkbreak[2]{#1}
\makeatother

\usepackage{geometry}
\geometry{%
	includeheadfoot,
	margin=1in
}

\def\TODO{{\bf ??? }}

\title{Argus: Deciding Questions about Events \\ (working paper) [DRAFT]}
\author{Petr Baudiš \\ Ailao}

\begin{document}
\maketitle

\begin{abstract}%
	We consider the problem of reliably answering yes/no questions
	about events based on news sources in settings of the Argus QA
	system and the Syphon question pre-validator.
	We give a brief overview of the relevant state-of-art methods
	in the academic field of Natural Language Processing,
	requirements for building a solid state-of-art system
	and difficulties we may anticipate in the road ahead.
	Based on this, we propose a basic architecture of the Argus
	system, roadmap to achieve baseline performance, and a basic
	version of the Syphon system.
\end{abstract}

\vspace{3ex}

{\itshape

In the working paper you should be sure to highlight the key aspects of uncertainty and anticipated difficulty.
	
And also, of course, the need for +alpha development - to whatever extent you
can detail what specifically is known and unknown that future development would
help clarify. Bringing in +that recent Cornell research might also be helpful
if you consider it valid (I know you were kind of skeptical of it).

Another thing I neglected to mention - I know they're not exactly the same, but it would also be very good to whatever extent you can to
point to YodaQA as a kind of preliminary proof of concept and include any mention you can of the lessons learned from building that and how those
lessons would/might come in to play with building the "yes/no/unclear" Argus option.
}

Our team at the Ailao startup, based at the 3C Group FEE CTU,
is composed of people working at the boundary of academic and commercial
sector and with a history of projects that involve
various aspects of Natural Language Processing ---
information retrieval, web scraping and information extraction,
deep learning and, most prominently in this context, question answering.
Our flagship project is the YodaQA system for answering factoid questions,
but we also built systems for automatic tagging of newsreel messages
or answering multiple-choice entrance exam type questions.

A natural question is whether we could simply apply our YodaQA system
on this task.  However, while there are some common analysis elements,
generating factoid answers is programatically quite a different task
from deciding whether a sentence is correct or incorrect.%
\footnote{A baseline support for answering yes / no questions is currently
under development within YodaQA as a student internship project, but it is
not finished yet and not meant to be highly reliable at this point.
The initial proposed method is at
\url{https://github.com/brmson/yodaqa/blob/f/textent/doc/ENTAILMENT.md}
but the difference from our scenario is that we are answering questions
on top of a fixed knowledge base, mainly structured one, rather than an
evolving news stream.  The baseline techniques are also so simple that
reusing implementation wouldn't help that much --- but reusing experience
definitely will.}
We believe the most efficient approach to take for a prototype is thus
to build a system from scratch, reusing mainly our expertise and possibly
some isolated external components (e.g.\ database interfaces or NLP annotators).

The rest of the paper is structured as follows.
In Sec.~\ref{qaml}, we rehash what does it involve to build Natural Language Processing
system that uses machine learning and is rigorously developed and measured.
In Sec.~\ref{ynml}, we look at current scientific concepts that relate
to the yes/no QA task at hand, and at their performance.
In Sec.~\ref{structure}, we explore the yes/no QA task in more detail,
identifying different types of questions and how to tackle each.
As a corollary, we explain how to sort the easy from the difficult
even before attempting an answer in Sec.~\ref{syphon}.
We sketch the system and its main concrete strategies based on these
ingredients in Sec.~\ref{system}.
Lastly in the Sec.~\ref{infra}, we give an overview of
the boring but most essential part of the system --- the infrastructure
that collects and manages our data, runs the experiments and interacts with the user.

\section{QA System in Machine Learning Context}
\label{qaml}

When building an NLP system that uses machine learning components,
we need a rigorous way to (i) train these components, and (ii) evaluate
the system performance.

Our NLP system is a \textit{classifier}, i.e.\ a program that takes
a sentence (and a large knowledge base) and classifies it as either
true or false.  The typical approach in such a scenario is building
a \textit{gold standard} dataset --- a set of questions, each annotated
with the correct answer (yes, no, unknown) and an accompanying knowledge
base that should, on its own, lend support to deciding these questions.%
\footnote{With a large enough knowledge base, this issue tends to vanish,
of course.}

In general, the bigger the dataset, the better.  Commonly used datasets
range in size from low hundreds%
\footnote{E.g.\ the \textit{Curated TREC QA Track Dataset} \url{https://github.com/brmson/dataset-curated-factoid/}}
to tens of thousands%
\footnote{E.g.\ the \textit{Large Movie Review Dataset} \url{http://ai.stanford.edu/~amaas/data/sentiment/}}
but we can certainly grow the dataset as needed and we will be able to identify
insufficient dataset size as a possible improvement factor during development.
We should start with at least 100 questions, for initial evaluation of our
baseline.

These questions are further split, at the very least to a \textit{training}
set and a \textit{testing} set (more fine splits are often desirable
with enough questions available).
To train even simple machine learning models, we should try hard to get
at least 300 questions, but to train some more advanced models, we need
to aim at figures like 6000 questions.

To get the dataset, the simplest option is to hire low-paid virtual assistants
to build it.  Aside of using common employment platforms, Amazon runs a
so-called Mechanical Turk service that allow humans to be employed easily
for these kinds of task.  Finally, it might be possible to build the dataset
semi-automatically by using some clever approaches --- this makes it easy
to build a large dataset, but holds the danger of noise (non-sensical questions
or wrong reference answers) and drifting off from realistic questions the
users will pose.  It would be an interesting problem / milestone on its own,
but something we might have to tackle.

Many of the models need another kind of dataset as well --- pairs of question
and news sentence that are annotated by whether the sentence carries the yes/no
answer for the question.  However, from a purely question-based dataset,
generating a medium-quality dataset like this should be straightforward,
and reviewing predefined annotations by humans is much easier and faster
than creating the annotations in the first place.

\textit{Our project proposal assumed that you would deliver such a dataset.
	We can certainly also arrange building such a dataset, in the vein
	of future-looking questions you have provided us, as an additionally
budgeted milestone.}

The main criterium for evaluating our system is obviously \textit{accuracy},
that is the percentage of questions our system gets right.  However, this
number does not capture questions where the system ``gives up'' and says
unknown; one possibility is to use a harmonic average of (i) the percentage
of questions the system gets right when it decides to answer, and (ii) the
percentage of questions the system decides to answer.%
\footnote{This combined percentage is called \textit{F1 score} in machine learning jargon.}
We can always change the threshold of confidence the system uses to decide
whether to answer to keep the accuracy of answered questions at the desired
score like 95\%, the caveat is that the system may refuse to answer almost
all questions at that point; therefore, this average metric is a better
option for judging the system performance.

Performance evaluation in the machine learning context has two clear limitations
that we are not sure how to overcome right now.  First, it is not clear how to
effectively evaluate Syphon performance when users are repeatedly adjusting
their queries to force them through; we'll come back to this below.
Second, we assume a \textit{non-adversarial scenario} where user questions
are expected to be unbiased and users are not actively trying to trick the system;
this assumption is not realistic, but we are not aware of any good answers science
has to this problem.  Out-of-the-loop safeguards like random manual review,
adaptive Syphon or large-scale consensus of diverse automata might be required.
Another possibility is that a group of ``white hat'' users will counter-analyze
trending questions for possible flaws of this nature and natural balance will
emerge in the ecosystem --- the motivation of these users being reputation and
credentials (user rating might be an excellent base for getting hired as a
proprietary analyst), or even bounties for bad questions.

\section{Machine Learning for Answering Yes/No Questions}
\label{ynml}

When considering the relevant state-of-art in Machine Learning (ML) and
Natural Language Processing (NLP), we will first give a short overview
of the recent advances in general natural language processing techniques,
then consider state-of-art in all the specific areas pertaining our question
answering task.

\subsection{Vector Embeddings}

Recent progress in NLP has been marked mainly by the proliferation of
so-called \textit{vector embeddings}, the most popular being called
\textit{word2vec}.  This approach stems from the so-called \textit{distributional semantics}
hypothesis, which poses that we can derive meanings of words purely
from the context they tend to appear in.  Therefore, each word is
associated with a list of $n$ real numbers (i.e.\ coordinates of
an $n$-dimensional vector) and these numbers are derived automatically
just from the context the words appear in.%
\footnote{The \textit{word2vec} method uses an idea called \textit{multi-task learning}
	which we will mention again later --- that we are trying to learn
	some easy-to-specify task and then we re-use the same model to
	solve some much harder problems.  Here, the $n$ real numbers
	come out from training a classifier that predicts the most likely
	next words to come given a context of (say, 100) preceding words;
	this so-called \textit{language model} task is useful e.g.\ in
	speech recognition or OCR.}
The interesting property is that the automatically assigned numbers
exhibit semantic properties in how they relate between words.
For example, if we do arithmetics on these word vectors and try
to compute e.g.\ $king + (woman - man)$, the nearest vector we reach
is $queen$, i.e.\ the gender transition is represented by an arrow
in our vector space (Fig.~\ref{fig:w2vg}).
Fig.~\ref{fig:w2ver} shows how are relationships between adjectives represented
while Fig.~\ref{fig:w2vc} shows mappings between coordinates of countries
and their capitals.  Let us emphasize again that these coordinates were
determined purely based on the context of the words (in Wikipedia or
millions of news articles), the system did not have any extra information
or databases available.

A lot of the current research focuses on the best ways to build up
vector representations of whole sentences and documents --- ranging
from simple averaging \cite{CNNSentClass,DefGen} (successful baselines)
to recurrent neural networks \cite{LISA,ShowAndTell}.
Many applications that rely on semantic understanding of word nuances
are popping up; this method became state-of-art for machine translation
\cite{LISA}, automatic image captioning \cite{ShowAndTell} and specific
types of question answering \cite{QANTA,DefGen,ReadAndComprehend}%
\footnote{The demo at \url{http://45.55.181.170/defgen/} is nice.}.
One open problem is efficient composition of vector embeddings for
common words with entities like numbers of proper names.

\subsection{NLP for Yes/No Questions}

The standard NLP task which is closest to answering yes/no questions
is so-called \textit{Recognizing Text Entailment} (RTE) task.
In the standard formulation,
we have a paragraph of context and a phrase that either is or isn't
\textit{entailed}, i.e. can be decided to be true based on the context.
The most prominent effort in this area is probably the
\textsc{Excitement EOP} academic project,%
\footnote{\url{http://hltfbk.github.io/Excitement-Open-Platform/}}
which is a full-fledged
RTE pipeline in Java that implements several algorithms
in a common framework.
Typical state-of-art RTE algorithms work on the principle of parse tree
alignment --- grammar dependency tree of the hypothesis and each context
sentence is compared and we try to learn which changes in the tree might
keep entailment.
But as the RTE problems are quite hard and practical applications are limited,
this is not a very lively area of research per se.

In the RTE field jargon, the Argus task is to detect whether either the question or
its negation is entailed from a given text article.  This involves
a mechanism to focus on the relevant parts of the article (if any)
and some sort of semantic analysis of these parts to determine
equivalence.
However, we want to argue that our task significantly differs from
the industry-standard benchmarks.  A few examples are listed in Fig.~\ref{fig:rte3}
(more examples include answering ``text comprehension'' problems from
English SAT exams%
\footnote{An example of this is: \url{http://nlp.uned.es/entrance-exams/}.})
and they demonstrate that the main focus of the RTE field is to enhance
deep computer understanding of written text and often the questions
require non-trivial inference from information-rich sentences.
Of course, this is a task a perfect, Syphon-less Argus type system
would eventually need to solve too, but our focus right now is on a much
easier task, where often many news headings will literally already contain
the answer we need.  Contrary to RTE, where the contextual paragraph is
typically a short snippet, our main focus is sieving the news source for
the relevant passages.

\begin{figure}
	\textbf{Context:} The sale was made to pay Yukos' US\$ 27.5 billion tax bill, Yuganskneftegaz was originally sold for US\$ 9.4 billion to a little known company Baikalfinansgroup which was later bought by the Russian state-owned oil company Rosneft. \\
	\textbf{Hypothesis:} Baikalfinansgroup was sold to Rosneft.

	\vspace{2ex}

	\textbf{Context:} US Steel could even have a technical advantage over Nucor since one new method of steel making it is considering, thin strip casting, may produce higher quality steel than the Nucor thin slab technique. \\
	\textbf{Hypothesis:} US Steel may invest in strip casting.

	\vspace{2ex}

	\textbf{Context:} The 'Club of Rome' was an appalling Joy Division soundalike band playing in Canberra, Australia in the early 1980's. They were foul, really. Boring, pretentious bunch of shore-starers who struggled to string a tune together, while all the while revelling in the reflected glory of late greats like the Birthday Party. Oh yeah, and they have the same name as some global think tank that deals with a variety of international political issues. \\
	\textbf{Hypothesis:} The 'Club of Rome' is a global think tank that deals with a variety of international political issues.

	\caption{Typical RTE pairs from academic datasets, mainly the ERT-3 challenge.}
	\label{fig:rte3}
\end{figure}

With this in mind, let us cover some recent successes mainly of
the vector embedding methods in some of the NLP fields related
to understanding of written text, but less demanding than the
full RTE task.  Let us cover a few popular benchmarks:

\begin{itemize}
	\item \textbf{Sentiment Analysis:} One of the chief benchmarks of
		language understanding, classifying texts (e.g.\ product
		reviews) as either positive and negative --- there are
		abundant datasets, eminent commercial applicability,
		the problem is very easy to phrase and judge, and yet
		simple models fail to solve it effectively.%
\footnote{Consider this utterance with a lot of positive words:
		\textit{This movie was actually neither that funny, nor super witty.}}
		The state-of-art (as far as we know) are the Recursive
		Neural Tensor Networks that model grammar-dependent
		transformations of vector embeddings, with
		85.4\% sentences correctly classified as positive or
		negative on the standard dataset. \cite{SentimentRNTN}
		A major relevance of this area for us is that detection
		of long-range negations or even irony is important in
		sentiment analysis.

	\item \textbf{Question Answering:} The problem we are ourselves
		tackling is question answering, however the primary focus
		there is the process of \textit{generating} facts while
		here we are in the business of \textit{verifying} them.
		Such a component would be obviously also useful in
		a question answering system, but e.g.\ in our \textit{YodaQA}
		system, we do not have such a (working) component yet.
		IBM Watson DeepQA system (famous from the Jeopardy! competition)
		has such a component described in \cite{WatsonEvidence};
		they propose also several simple techniques we plan to
		use as our baselines.

		Nevertheless, the question answering task is large and
		consists of many specific sub-tasks.  Let us detail two
		that would be relevant for us.

	\item \textbf{Question Type Classification:} This task can serve
		as a demo of state-of-art capability to classify a sentence
		into a specific category, in this case whether the question
		is, say, about a number or an entity.  State-of-art approaches
		(93.2\% accuracy)
		include neural networks based on vector embeddings \cite{QtcDCNN}.

	\item \textbf{Answer Sentence Selection:} A significant portion
		of our problem will be identifying news articles
		and sentences relevant to our questions in large swathes
		of irrelevant data.  The sentence selection sub-task is
		about identifying sentences that bear an answer to the
		given question.%
\footnote{This is a relatively hard problem; the
		classifier, given \textit{What does the Peugeot company manufacture?}
		must select \textit{Peugeot and Rover last month ended an agreement under which Rover distributed Peugeot cars through its dealers in Japan.}
	but reject \textit{Renault and Peugeot are getting less integrated and becoming primarily designers and assemblers.}}
		The state-of-art method \cite{Yu2014Deep} uses vector embeddings
		and when sorting sentences by relevance, achieves a mean rank
		of about 1.2.%
\footnote{To clarify, this is a reciprocial of Mean Reciprocial Rank.}
		We have built our own implementation of this method
		which reproduces the results and which we could easily
		reuse in Argus.%
\footnote{\url{https://github.com/brmson/Sentence-selection}}
		The downside is that we would need a dataset which has
		each sentence manually labelled by whether it answers
		the posed yes/no question.

\end{itemize}

To conclude, the yes/no question answering problem in a scenario that
would make it easily applicable to Argus has been left almost untackled
in the literature.  However, several recent benchmarks in related areas
highlight some techniques which should be highly successful on our task.
The state-of-art algorithms often involve vector embeddings of words,
the caveat of which is the requirement of extensively labelled data set.

\section{Structure of Our Problem}
\label{structure}

\begin{figure}
\begin{verbatim}
Did Hillary Clinton become Presidemt?
Did Scott Walker become US President?
Did Martin O’Malley become US President?
Was Obamacare dismantled by the Supreme Court?
Was Saudi Arabia’s monarchy be dethroned in a coup d’etat?
Did Iran test detonate a nuclear bomb"
Did Israel attack Iran?
Did Germany ban the PEDIGA movement?
Did the US dollar collapse by at least 50\%?
Did the euro collapse another 50\%?
Did Germany abandon the euro?
Did the EU slide into a new recession?
Was a miracle cure for diabetes discovered?
Did a real blizzard lead to power cuts for more than 10 million Americans?
Did Sears declare bankruptcy?
\end{verbatim}
	\caption{Sample questions as provided by the customer.}
	\label{fig:sampleq}
\end{figure}

Clearly, not all yes/no questions are created equal:
Some are easy to understand, others are hard.
Some are easy to find an answers, others will produce just a smidgen of mentions in the knowledge base.
Some are quantitatively unclear and answers might be triggerred by published subjective opinions.
Let us explore what these distinctions mean,
since recognizing them and rejecting answering
the hard questions outright is our key to achieving a high precision.
Fig.~\ref{fig:sampleq} contains a list of sample questions (forward-looking as of now).

Let us group the questions to several classes in terms of how easy to automatically judge they are:

\begin{itemize}
	\item Clear-cut questions: \textbf{Did X Y become President?},
		\textbf{Did Iran test detonate a nuclear bomb?},
		\textbf{Did Germany ban the PEDIGA movement?}

		These should be generally easy to parse by Syphon
		and to answer unambiguously from news sources.

	\item Fuzzy-matching questions: \textbf{Was Saudi Arabia’s monarchy be dethroned in a coup d’etat?}
		\textbf{Did Germany abandon the euro?},
		\textbf{Did Sears declare bankruptcy?}

		These are about unambiguous facts and should pass Syphon,
		but simple text matching strategies might fail when
		answering the question - the system needs to match
		``revolution'' to ``dethroned in a coup d'etat'',
		many news titles would use ``leave the eurozone''
		instead of ``abandon the euro'' and ``declare bankrupcy''
		is the same as e.g.\ ``go bankrupt''.

		Based on results like TODO,
		we can predict that many of the simpler paraphrases
		would be covered by vector embeddings%
\footnote{There is also a Wordnet dictionary that covers many synonymic relationships of individual words, but extending it to verb/noun combinations is less trivial.}
		but this is unlikely to solve the ``abandon the euro''.
		We can either rely on stricter Syphon
		(we could suggest templates like ``starts'' and ``stops''
		together with knowledge base attributes),
		or simply hope that the newspapers will use so varied
		phrasings that the one particularly in the question
		will still appear often enough; the alpha prototype
		should shed more light on this.

		Also, another tricky part is that \textbf{Sears} is name
		of many legal entities; do we mean the department store
		chain, \textbf{Sears Holdings} or some sister company?
		However, Syphon can catch ambiguous references like this.

	\item Questions requiring inference: \textbf{Did the US dollar collapse by at least 50\%?},
		\textbf{Did the euro collapse another 50\%?},
		\textbf{Did a real blizzard lead to power cuts for more than 10 million Americans?}

		Answering these questions with simple text matching
		is unlikely to produce any results.  Syphon shouldn't
		be able to decompose this question to simple elements
		and relationships between these and so shouldn't let
		pass these questions through.

	\item Subjective questions: \textbf{Was Obamacare dismantled by the Supreme Court?},
		\textbf{Did Israel attack Iran?},
		\textbf{Did the EU slide into a new recession?},
		\textbf{Was a miracle cure for diabetes discovered?}

		These questions are particularly tricky as filtering
		them with Syphon may be problematic.  Entity X attacking
		entity Y seems quite unambiguous and well-defined.
		On the other hand, the definition of ``attack'' may
		vary widely\footnote{Israel is a good example as the country is known for executing military and intelligence operations covertly and avoiding admission of involvement.} --- from bombing of specific site
		to a full-scale military attack.

		We don't have a good solution to this class of questions,
		except warning users that the act of ``dismantling''
		simply means that enough commentators consider the court
		judgement to be this, the same with ``recession'' or what
		is a ``miracle cure'' (we already have several, or none).
\end{itemize}

Aside of questions posed, another important aspect is what data sources
we will use for answering them.  One natural idea which we are working
with from the beginning is scanning news feeds from a variety of reputable
news sites and mining answers from natural text.
However, as an alternative, we could leverage collaboratively maintained
databases.  The most prominent of these is WikiData, which is becoming the
base of many infoboxes at Wikipedia and has clear semantic structure.
We would have to establish some trustworthiness metrics (i.e.\ consider
only high-profile concepts and require the value to be unchallenged in
the database for long enough), but the advantage would be a fairly clear
signal for our system, e.g.\ in the form of clearly valued \textit{currency}
property for the \textit{Germany} entity.

\section{How Can We Know What Do We Know?}
\label{syphon}

The initial phase of the Argus system needs to be the Syphon sub-system
which carries the role of building up a semantically rich question
representation from the natural language input, rejecting answers where
such a representation cannot be unambiguously built, and providing
some sort of templating and/or auto-completion capabilities for partial
questions.  Even when a representation is built and passed, an important
aspect for trustworthiness of the system is that the representation is
shown (in user-friendly form) to Argus users as a feedback and a clear
signal of how the system will decide the answers.

To maintain high precision of answers, the Syphon system should be
conservative about the questions it allows.  For the baseline system,
in light of the analysis of question classes above,
we want to propose a simple Syphon which scans the question
for named entities%
\footnote{\textit{Named entity} in NLP parlance is something that is
	not a plain English word carrying meaning, but e.g.\ a proper
	name or a numerical value (date, monetary amount, \dots).
	There exist specialized scanners that extract named entities
	of specific types; as a simple baseline, we may consider
	all strings that are titles of Wikipedia articles (or redirects,
	that is essentially unique aliases).  For example
	\textit{Hillary Clinton} and \textit{President of the United States}
	both are titles of such articles.}
	and verbs which describe clear relationships
	and verifies that no words without an assigned role remain
	in the question.  This should keep most of the sample questions
	covered and with a clear interpretation.

\section{A Concrete Proposal}
\label{system}

Concrete system proposal: Syphon that detects named entities (enwiki titles) and verbs, rejects if anything extra is present.
Get pre-processed version from syphon.
Scan news titles (and possibly structured databases), model entailment (yes, unknown, now) for each,
use relative counts (normalized by general named entity occurrence) to estimate probability.

initial knowledge base: guardian api? wikidata?

baseline entailment method: bag-of-words, use Wordnet synsets.
Deep anssentsel would be easy, but needs a dataset of at least a few thousand question, sentence pairs - use multitask learning?

\section{Implementation and Infrastructure}
\label{infra}

technical details --- python, database used, etc.

news gathering etc.

distributed system prospects.

\bibliographystyle{plainnat}
\bibliography{qa}

\end{document}
